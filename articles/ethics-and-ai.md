## Ethics & AI

---
| Field | Value |
|----|----|
| Writer | Arpita Saggar - MCA I yr|
| Editor | Swati Gautam			   |
| Status | -                       |
| Plagiarism| None. [Report](./plag-reports/plag-ethics-and-ai.pdf) | 

---
The pursuit of building machines that can replicate true human behaviour has been a relentless and unfruitful one for the past few decades, yet there is evidence to suggest that this dream may become a reality far sooner than one might imagine. While the task of successfully passing a Turing test remains the hardest challenge for building such a system, the more pressing matter is how we control and regulate it. If indeed the ambition of an independent, self-conscious AI is realized, what ethics should govern its operations? Which morals apply to the machine – those of its designer or those of its consumer? 
The 2012 film ‘Robot and Frank’ offers some insight (Spoiler Alert!). The film tells the story of retired cat burglar Frank, who suffers from dementia, and is given a robot butler to help him around the house. Eventually, Frank trains his robot to pick locks and help him commit robberies. The robot agrees to be an accessory to theft, simply because planning the robberies helps stimulate his owner’s otherwise deteriorating brain, even helping him get rid of evidence in order to avoid prosecution. Would such a situation be considered ‘good judgement’? The robot did act in the best interests of its owner. But perhaps various law enforcement agencies would disagree. 

Teaching morals to a machine is an arduous task, because humans themselves stand on shaky ground when it comes to describing what constitutes a ‘sound’ moral character. Our decisions are often governed by our worldliness and emotions, as opposed to machines which use cost-benefit analysis. How then, does one combine the best of both worlds to build an AI that can integrate itself seamlessly into our society? The difficulty lies in the approach that should be adopted to replicate human morality.
Researchers at MIT have developed ‘Moral Machine’, a platform that crowdsources human perspectives pertaining to moral dilemmas, while developers of the ‘Quixote’ AI at the Georgia Institute of Technology have turned to literary works to teach machines socially acceptable behaviour. Can either of these approaches be deemed ideal? We’ve already established that human morality is fickle. But what about fiction? Is the protagonist of every story perched on the moral high ground? A quick look at Macbeth or The Picture of Dorian Gray will prove that untrue. Such approaches are sensible but are not without flaw. Indeed there is no foolproof technique (yet) that can master the impartment of ethics. Yet, the situation isn’t entirely hopeless. Treating an AI as an autonomous, self-thinking agent rather than a program might prove beneficial. This would mean not just maintaining and troubleshooting a machine, but also nurturing it like a fellow human. 


The question remains if there will ever be a perfectly moral AI, especially since there isn’t a human with perfect morals. If the creator isn’t perfect, does that imply that neither will be the creation? Perhaps some questions are best left unanswered. As machines become increasingly ubiquitous in our daily lives, the price of absent morals could negatively affect the lives of billions. Forging true artificial intelligence will by no means be an easy task, but nonetheless one worth pursuing
